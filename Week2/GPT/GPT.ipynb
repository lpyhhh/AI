{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3162376b",
   "metadata": {},
   "source": [
    "目标：  \n",
    "1 学会搭建基础模型  \n",
    "2 相关代码能默写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b14a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a447670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 model\n",
    "tokenizer=GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model=GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5108389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm sorry,but my name is py,please tell me iay,I'm\n"
     ]
    }
   ],
   "source": [
    "# 使用\n",
    "prompt = \"my name is py,please tell me \"\n",
    "inputs=tokenizer(prompt,return_tensors=\"pt\")\n",
    "\n",
    "outputs=model.generate( #hugging 用于文本生成的\n",
    "    **inputs,\n",
    "    max_new_tokens=600, #生成的token数\n",
    "    temperature=0.8,#生成的内容的随机性，是否完全与问题相关\n",
    "    top_p=0.9,#概率总和达 90% 的候选词中选择词\n",
    "    do_sample=True,#启用随机采样模式（配合temperature和top_p），生成更自然的文本（而非贪心选择）\n",
    "    pad_token_id=tokenizer.eos_token_id,#指定填充符 ID 为结束符 ID\n",
    ")\n",
    "out=tokenizer.decode(outputs[0],skip_special_tokens=True) #解码时对于无关字符不展示\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
