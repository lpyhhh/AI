{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e69eaee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch>=2.1.0 torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2efefb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/software/miniconda3/envs/week2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe89882d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 4000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#1 data\n",
    "dataset=load_dataset(\"imdb\")\n",
    "train_dataset=dataset[\"train\"].shuffle(seed=42).select(range(4000))\n",
    "test_dataset=dataset[\"test\"].shuffle(seed=42).select(range(4000))\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d55dce61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask']\\ninput_ids：文本对应的数字编码\\nattention_mask：标记哪些位置是实际文本（1），哪些是填充（0）\\n可能还有token_type_ids（针对某些模型的句子类型标记）\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2 处理导入\n",
    "#token输入序列\n",
    "tokenizer=BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"],padding=\"max_length\",truncation=True) #如何对句子进行token化 填充到指定长度，trunca截断\n",
    "\n",
    "train_dataset=train_dataset.map(tokenize_function,batched=True)\n",
    "test_dataset=test_dataset.map(tokenize_function,batched=True)\n",
    "#print(train_dataset[1])\n",
    "\"\"\"\n",
    "#['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "input_ids：文本对应的数字编码\n",
    "attention_mask：标记哪些位置是实际文本（1），哪些是填充（0）\n",
    "可能还有token_type_ids（针对某些模型的句子类型标记）\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d239e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练参数配置\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model\",  # 训练输出文件（模型、日志等）的保存目录\n",
    "    eval_strategy=\"epoch\",  # 评估策略：每完成一个训练轮次(epoch)后执行评估\n",
    "    save_strategy=\"best\",  # 模型保存策略：\"no\"表示不自动保存模型checkpoint\n",
    "    save_total_limit=3, #最多保存三个模型\n",
    "    per_device_train_batch_size=32,  # 单设备上的训练批次大小（每个批次包含8个样本）\n",
    "    per_device_eval_batch_size=32,  # 单设备上的评估批次大小（每个批次包含8个样本）\n",
    "    num_train_epochs=6,  # 训练总轮次：整个数据集将被训练2遍\n",
    "    learning_rate=2e-5,  # 学习率：BERT微调常用的学习率（2e-5即0.00002）\n",
    "    logging_dir=\"./logs\",  # 训练日志的保存目录（可用于TensorBoard可视化）\n",
    "    logging_steps=50,  # 日志记录频率：每训练50个步骤(step)记录一次日志\n",
    "    load_best_model_at_end=True,#训练结束后加载验证集效果最好的模型\n",
    "\n",
    "    metric_for_best_model=\"accuracy\",  # 定义\"最佳模型\"的评判指标（需与compute_metrics对应）\n",
    "    greater_is_better=True,  # 指标是否越大越好（accuracy越大越好，loss则设为False）\n",
    "    weight_decay=0.01,  # 权重衰减（防止过拟合，BERT微调常用）\n",
    "    warmup_steps=500,  # 学习率预热步数（稳定训练初期的梯度）\n",
    "    fp16=True,  # 若设备支持，开启混合精度训练（加速训练并节省显存）\n",
    "    report_to=\"tensorboard\",  # 日志报告到TensorBoard（默认也是，但显式指定更清晰）\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e96e0d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 07:22, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>0.491450</td>\n",
       "      <td>0.817000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.282500</td>\n",
       "      <td>0.275396</td>\n",
       "      <td>0.888750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.223000</td>\n",
       "      <td>0.243163</td>\n",
       "      <td>0.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.152300</td>\n",
       "      <td>0.417580</td>\n",
       "      <td>0.868250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.068300</td>\n",
       "      <td>0.329303</td>\n",
       "      <td>0.908750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.340508</td>\n",
       "      <td>0.920750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.34050774574279785,\n",
       " 'eval_accuracy': 0.92075,\n",
       " 'eval_runtime': 17.1888,\n",
       " 'eval_samples_per_second': 232.71,\n",
       " 'eval_steps_per_second': 7.272,\n",
       " 'epoch': 6.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3 model\n",
    "from sklearn.metrics import accuracy_score\n",
    "model=BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "#计算准确率\n",
    "def compute_metrics(eval_pred):\n",
    "    logits,labels=eval_pred #数据集 结果和标签\n",
    "    preds=torch.argmax(torch.tensor(logits),dim=-1) #激活函数\n",
    "    acc=accuracy_score(labels,preds)\n",
    "    return {\"accuracy\": acc}\n",
    "    \"\"\"\n",
    "    logits：模型的原始输出（通常是未经过 softmax 激活的张量），形状一般为 (batch_size, num_classes)\n",
    "    torch.argmax(..., dim=-1)：在最后一个维度（即类别维度）上取最大值的索引，也就是预测的类别\n",
    "    \"\"\"\n",
    "\n",
    "#模型计算\n",
    "trainer=Trainer( #模型，参数，数据集，准确率判断。 为什么没有优化函数呢？直接在model中定义好\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
