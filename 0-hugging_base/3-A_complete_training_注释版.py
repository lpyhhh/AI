#/usr/bin/env python
'''
ç¬¬ä¸‰ç« å‰ï¼Œå…¨éƒ¨çš„æµç¨‹
'''
### å¯¼å…¥åŒ… æ•°æ®å¤„ç†
from datasets import load_dataset
from transformers import AutoTokenizer,DataCollatorWithPadding

raw_datasets=load_dataset("glue","mrpc")
checkpoint="bert-base-uncased"
tokenizer=AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example['sentence1'],example['sentence2'],truncation=True)

tokenized_datasets=raw_datasets.map(tokenize_function, batched=True)#å‡½æ•°çš„å¦ä¸€ç§ä½¿ç”¨æ–¹å¼ï¼štokenize_function()ï¼ŒPython ä¼šå…ˆæ‰§è¡Œè¿™ä¸ªå‡½æ•°ï¼ˆä½†æ­¤æ—¶æ²¡æœ‰ä¼ å…¥example å‚æ•°ï¼Œä¼šç›´æ¥æŠ¥é”™ï¼‰ï¼Œè¿™æ˜¾ç„¶ä¸ç¬¦åˆéœ€æ±‚ã€‚mapå…ˆæ‰¹é‡å¤„ç†æ•°æ®ï¼Œå†ä¼ é€’ç»™å‡½æ•°
data_collator=DataCollatorWithPadding(tokenizer=tokenizer)
#print(tokenized_datasets)#æŸ¥çœ‹æ•°æ®å¤„ç†ç»“æœ

#è®­ç»ƒå‰æ•°æ®æ¸…æ´—ï¼šæŒ‡å®šæˆ‘ä»¬æƒ³è¦çš„æ•°æ® åˆ é™¤ é‡å‘½å æ ¼å¼ æŸ¥çœ‹ ["attention_mask", "input_ids", "labels", "token_type_ids"]
tokenized_datasets=tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets=tokenized_datasets.rename_column("label","labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names #æŸ¥çœ‹å¤„ç†åè®­ç»ƒé›†åŒ…å«çš„åˆ—åï¼ˆç›¸å½“äº â€œæ•°æ®é›†çš„è¡¨å¤´â€ï¼‰
#print(tokenized_datasets)

### æ•°æ®å¯¼å…¥é›†è®¾ç½®
from torch.utils.data import DataLoader
train_dataloader=DataLoader(tokenized_datasets['train'],shuffle=True,batch_size=8,collate_fn=data_collator)
eval_dataloader=DataLoader(tokenized_datasets['validation'],shuffle=True,batch_size=8,collate_fn=data_collator)
#æ£€æŸ¥æ•°æ®æ€§çŠ¶
for batch in train_dataloader:
    break
for k,v in batch.items():
    #print(v.shape)
    break
"""
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
ä¸ºä»€ä¹ˆæ˜¯([8, 65])ï¼Ÿ
8 æ¯ä¸ªbatchå¤§å°ï¼Œ batch_size=8
65 paddingé•¿åº¦
"""
### æ¨¡å‹æ„å»º
from transformers import AutoModelForSequenceClassification
model=AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=2)

output=model(**batch)
#print(output.loss,output.logits.shape)

### è®­ç»ƒæ¨¡å‹
#ä¼˜åŒ–ç‡ å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¦‚ä½•å®šä¹‰ï¼Ÿå¦‚ä½•ä½¿ç”¨ï¼‰
#ä¼˜åŒ–ç‡ ç›®çš„ï¼šæ ¹æ®æŸå¤±å‡½æ•°è®©lossæœ€å°åŒ–ã€‚ ç”¨æ³•ï¼šæ¥å—loss.backward()
#å­¦ä¹ ç‡è°ƒåº¦å™¨=æ­¥é•¿

#å­¦ä¹ ç‡é€æ­¥é™ä½ï¼Œç›®çš„æ˜¯ä¸ºäº†é˜²æ­¢ è·³è¿‡æœ€ä¼˜ç‚¹ã€‚
'''
åŒºé—´ï¼š3 ä¸ª epochs Ã— æ¯ä¸ª epoch çš„ batch æ•°
å­¦ä¹ ç‡ä» 5e-5 çº¿æ€§é™åˆ° 0ï¼Œè¿™ä¸ªè¿‡ç¨‹æ˜¯åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­å®Œæˆçš„ã€‚
è¦ç®—å‡ºâ€œæ€»å…±å¤šå°‘æ­¥â€ï¼Œå°±ç”¨ 3 ä¸ª epochs Ã— æ¯ä¸ª epoch çš„ batch æ•°ï¼Œè¿™æ ·è°ƒåº¦å™¨æ‰çŸ¥é“â€œæ¯ä¸€æ­¥â€è¯¥æŠŠå­¦ä¹ ç‡é™åˆ°å¤šå°‘ã€‚
'''
from torch.optim import AdamW
from transformers import get_scheduler

optimizer=AdamW(model.parameters(),lr=5e-5)#æ¨¡å‹é‡Œæ‰€æœ‰å¯è®­ç»ƒå‚æ•°äº¤ç»™ AdamW
#TrainingArguments æ¨¡å‹æ‰€æœ‰çš„å‚æ•°

num_epochs=3
num_training_steps=num_epochs*len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",                       # è°ƒåº¦å™¨ç±»å‹ï¼šçº¿æ€§è¡°å‡
    optimizer=optimizer,            # ç»™å“ªä¸ªä¼˜åŒ–å™¨è°ƒå­¦ä¹ ç‡
    num_warmup_steps=0,             # çƒ­èº«æ­¥æ•°ï¼Œè¿™é‡Œè®¾ 0 è¡¨ç¤ºä¸éœ€è¦
    num_training_steps=num_training_steps,  # æ€»æ­¥æ•°ï¼ˆç»ˆç‚¹ï¼‰
)
#æ­¥æ•°print(num_training_steps)

### è®­ç»ƒ
import torch
device=torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
#print(device)
'''
å¯¼å…¥tqdm
å®šä¹‰æ€»è½®æ•°

è®­ç»ƒæ¨¡å¼
epochå¾ªç¯
    batchå¾ªç¯
        batchæ•°æ®
        modelè®­ç»ƒç»“æœ
        è®¡ç®—æŸå¤±
        åå‘ä¼ æ’­

        æ›´æ–°æƒé‡ + è°ƒå­¦ä¹ ç‡ + æ¸…é›¶æ¢¯åº¦
        è¿›åº¦æ¡
'''
from tqdm import tqdm

progress_bar=tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch={k:v.to(device) for k,v in batch.items()}
        outputs=model(**batch)
        loss=outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

### è¯„ä¼°
# metric.compute() æ–¹æ³•
'''
evalè¯„ä¼°

å¯¼å…¥è¯„æµ‹æŒ‡æ ‡
æ¨¡å‹evalè¯„ä¼°æ¨¡å¼
for evalæ•°æ®é›†
    æ•°æ®æ”¾åˆ°GPU
    å…³é—­æ¨ç†ï¼Œåªè¦output
    æŠŠlogitè½¬åŒ–ä¸ºç±»åˆ«
    é¢„æµ‹å’ŒçœŸå€¼ä¿å­˜
æœ€ç»ˆæŒ‡æ ‡
'''
import evaluate

metric=evaluate.load("glue","mrpc")
model.eval()
for batch in eval_dataloader:
    batch={k:v.to(device) for k,v in batch.items()}
    with torch.no_grad():
        outputs=model(**batch)
    logits=output.logits
    predictions=torch.argmax(logits,dim=-1)
    metric.add_batch(predictions=predictions,references=batch["labels"])

print(metric.compute())

model.save_pretrained("/home/ec2-user/project/AI/0-hugging_base/huggingface_test_model")
tokenizer.save_pretrained("/home/ec2-user/project/AI/0-hugging_base/huggingface_test_model")

### å¤šè®¾å¤‡è¿è¡Œ
'''
accelerateåŠ é€Ÿè®¡ç®—ç±»
å¯¼å…¥å‡½æ•°ï¼Œå®ä¾‹åŒ–
åˆå§‹åŒ–å‡†å¤‡ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(train_dataloader, eval_dataloader, model, optimizer)
è®­ç»ƒæ•°æ®è®¾ç½®batch in train_dataloader:
accelerator.backward(loss)

å¦‚ä½•è¿è¡Œ
accelerate config
accelerate launch train.py

'''

#tesk SST-2æ•°æ®é›†ä½¿ç”¨+

"""
äº†è§£äº† Hub ä¸­çš„æ•°æ®é›†
å­¦ä¹ äº†å¦‚ä½•åŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›†ï¼ŒåŒ…æ‹¬ä½¿ç”¨åŠ¨æ€å¡«å……å’Œæ•°æ®æ•´ç†å™¨
å®ç°ä½ è‡ªå·±çš„æ¨¡å‹å¾®è°ƒå’Œè¯„ä¼°
å®ç°äº†ä¸€ä¸ªè¾ƒä¸ºåº•å±‚çš„è®­ç»ƒå¾ªç¯
ä½¿ç”¨ ğŸ¤— Accelerate è½»æ¾è°ƒæ•´ä½ çš„è®­ç»ƒå¾ªç¯ï¼Œä½¿å…¶é€‚ç”¨äºå¤šä¸ª GPU æˆ– TPU
"""