{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/usr/bin/env python\n",
    "\"\"\"\n",
    "Linux如何声明环境变量\n",
    "三种：临时 全局 全用户\n",
    "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk\n",
    "export PATH=/usr/local/bin:$PATH\n",
    "\n",
    "vim ~/.bashrc\n",
    "\n",
    "vim /etc/profile\n",
    "export JAVA=\n",
    "\n",
    "#? 有问题\n",
    "\"\"\"\n",
    "\n",
    "#1 加载模型 pipeline\n",
    "\"\"\"\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")\n",
    "print(generator)\n",
    "\"\"\"\n",
    "#2模型内部是如何运行的？\n",
    "#token+model+head_model+pretraining\n",
    "\n",
    "# myself\n",
    "#token,model,head_model,\n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "checkpoint=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "token=AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs=token(raw_inputs,padding=True,truncation=True,return_tensors=\"pt\") #长度填充，位置填充\n",
    "#print(inputs)\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model=AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "out=model(**inputs)\n",
    "print(out.logits)\n",
    "\n",
    "pred=torch.nn.functional.softmax(out.logits,dim=-1)\n",
    "print(pred)\n",
    "\n",
    "#model\n",
    "#模型内部是如何组成的？\n",
    "from transformers import BertConfig, BertModel\n",
    "contig=BertConfig()\n",
    "#model=BertModel(contig)#只加载配置模型\n",
    "model=BertModel.from_pretrained(\"bert-ase-cased\")#经过预训练\n",
    "\n",
    "model.save_pretrained(\"../model/bert\")\n",
    "\n",
    "# 标记器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "序列-子词\n",
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple', ',', 'ni', 's', '##hi', 'g', '##e', 'da', 's', '##ha', 'bi']\n",
      "子词-矩阵\n",
      "[7993, 170, 13809, 23763, 2443, 1110, 3014, 117, 11437, 188, 3031, 176, 1162, 5358, 188, 2328, 16516]\n",
      "解码\n",
      "Using a Transformer network is simple, ni shi ge da sha bi\n"
     ]
    }
   ],
   "source": [
    "#2.4 标记器\n",
    "'''\n",
    "目的：输入序列如何转化为矩阵\n",
    "model input 是矩阵\n",
    "token 把输入数据 转化为 矩阵\n",
    "三种：基于 单词 字母 子词subword\n",
    "'''\n",
    "#加载tokenizer并保存到本地\n",
    "from transformers import BertTokenizer\n",
    "token=BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "token(\"Using a Transformer network is simple\",\",ni shi ge da sha bi\")\n",
    "token.save_pretrained(\"./model_test\")\n",
    "\n",
    "#编码\n",
    "#序列-子词\n",
    "from transformers import AutoTokenizer\n",
    "token=AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "seq=\"Using a Transformer network is simple\",\",ni shi ge da sha bi\"\n",
    "tokens=token.tokenize(seq) #\n",
    "print(\"序列-子词\")\n",
    "print(tokens)\n",
    "ids=token.convert_tokens_to_ids(tokens) #\n",
    "print(\"子词-矩阵\")\n",
    "print(ids)\n",
    "decode=token.decode(ids) #\n",
    "print(\"解码\")\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n"
     ]
    }
   ],
   "source": [
    "#课后作业\n",
    "#1 浅析模型的基本使用流程 从token model head_model \n",
    "\"\"\"\n",
    "基本使用流程\n",
    "输入句子sequence=\n",
    "把句子转化为矩阵 auto（）\n",
    "使用模型把矩阵后的序列进行预测 model（）\n",
    "加上模型头 对句子类型 进行分类\n",
    "\"\"\"\n",
    "#模型基本使用\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "token=tokenizer(sequence,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "pre=model(**token)\n",
    "print(pre)\n",
    "\n",
    "#加token后\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "token=AutoTokenizer.from_pretrained(checkpoint)\n",
    "model=AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "tokens=token.tokenize(sequence)\n",
    "ids=token.convert_tokens_to_ids(tokens)\n",
    "#这里面保存的是单个句子的矩阵化，而不是 model所需要的信息，三维矩阵\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#3微调模型\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 和之前一样\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "\n",
    "batch=tokenizer(sequences,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "#加上标签\n",
    "batch[\"labels\"]=torch.tensor([1,1])\n",
    "#print(batch)\n",
    "#优化模型的参数\n",
    "optimizer=AdamW(model.parameters()) #优化器，优化模型所有参数\n",
    "loss=model(**batch).loss #输入数据与训练\n",
    "loss.backward() #反向传播算法，自动计算损失函数对所有可训练参数的梯度\n",
    "optimizer.step() #更新参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 3668\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 408\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 1725\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#3.1 数据集加载\n",
    "#!pip install datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets=load_dataset(\"glue\",\"mrpc\")\n",
    "#print(raw_datasets)\n",
    "#数据集情况：训练集、验证集和测试集；每个集合有四列：句子1 2 标签 分段\n",
    "\n",
    "#查看数据情况\n",
    "raw_train=raw_datasets['train']\n",
    "print(raw_train[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
