{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/usr/bin/env python\n",
    "\"\"\"\n",
    "Linux如何声明环境变量\n",
    "三种：临时 全局 全用户\n",
    "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk\n",
    "export PATH=/usr/local/bin:$PATH\n",
    "\n",
    "vim ~/.bashrc\n",
    "\n",
    "vim /etc/profile\n",
    "export JAVA=\n",
    "\n",
    "#? 有问题\n",
    "\"\"\"\n",
    "\n",
    "#1 加载模型 pipeline\n",
    "\"\"\"\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")\n",
    "print(generator)\n",
    "\"\"\"\n",
    "#2模型内部是如何运行的？\n",
    "#token+model+head_model+pretraining\n",
    "\n",
    "# myself\n",
    "#token,model,head_model,\n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "checkpoint=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "token=AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs=token(raw_inputs,padding=True,truncation=True,return_tensors=\"pt\") #长度填充，位置填充\n",
    "#print(inputs)\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model=AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "out=model(**inputs)\n",
    "print(out.logits)\n",
    "\n",
    "pred=torch.nn.functional.softmax(out.logits,dim=-1)\n",
    "print(pred)\n",
    "\n",
    "#model\n",
    "#模型内部是如何组成的？\n",
    "from transformers import BertConfig, BertModel\n",
    "contig=BertConfig()\n",
    "#model=BertModel(contig)#只加载配置模型\n",
    "model=BertModel.from_pretrained(\"bert-ase-cased\")#经过预训练\n",
    "\n",
    "model.save_pretrained(\"../model/bert\")\n",
    "\n",
    "# 标记器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "序列-子词\n",
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple', ',', 'ni', 's', '##hi', 'g', '##e', 'da', 's', '##ha', 'bi']\n",
      "子词-矩阵\n",
      "[7993, 170, 13809, 23763, 2443, 1110, 3014, 117, 11437, 188, 3031, 176, 1162, 5358, 188, 2328, 16516]\n",
      "解码\n",
      "Using a Transformer network is simple, ni shi ge da sha bi\n"
     ]
    }
   ],
   "source": [
    "#2.4 标记器\n",
    "'''\n",
    "目的：输入序列如何转化为矩阵\n",
    "model input 是矩阵\n",
    "token 把输入数据 转化为 矩阵\n",
    "三种：基于 单词 字母 子词subword\n",
    "'''\n",
    "#加载tokenizer并保存到本地\n",
    "from transformers import BertTokenizer\n",
    "token=BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "token(\"Using a Transformer network is simple\",\",ni shi ge da sha bi\")\n",
    "token.save_pretrained(\"./model_test\")\n",
    "\n",
    "#编码\n",
    "#序列-子词\n",
    "from transformers import AutoTokenizer\n",
    "token=AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "seq=\"Using a Transformer network is simple\",\",ni shi ge da sha bi\"\n",
    "tokens=token.tokenize(seq) #\n",
    "print(\"序列-子词\")\n",
    "print(tokens)\n",
    "ids=token.convert_tokens_to_ids(tokens) #\n",
    "print(\"子词-矩阵\")\n",
    "print(ids)\n",
    "decode=token.decode(ids) #\n",
    "print(\"解码\")\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n"
     ]
    }
   ],
   "source": [
    "#课后作业\n",
    "#1 浅析模型的基本使用流程 从token model head_model \n",
    "\"\"\"\n",
    "基本使用流程\n",
    "输入句子sequence=\n",
    "把句子转化为矩阵 auto（）\n",
    "使用模型把矩阵后的序列进行预测 model（）\n",
    "加上模型头 对句子类型 进行分类\n",
    "\"\"\"\n",
    "#模型基本使用\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "token=tokenizer(sequence,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "pre=model(**token)\n",
    "print(pre)\n",
    "\n",
    "#加token后\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "token=AutoTokenizer.from_pretrained(checkpoint)\n",
    "model=AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "tokens=token.tokenize(sequence)\n",
    "ids=token.convert_tokens_to_ids(tokens)\n",
    "#这里面保存的是单个句子的矩阵化，而不是 model所需要的信息，三维矩阵\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#3微调模型\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 和之前一样\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "\n",
    "batch=tokenizer(sequences,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "#加上标签\n",
    "batch[\"labels\"]=torch.tensor([1,1])\n",
    "#print(batch)\n",
    "#优化模型的参数\n",
    "optimizer=AdamW(model.parameters()) #优化器，优化模型所有参数\n",
    "loss=model(**batch).loss #输入数据与训练\n",
    "loss.backward() #反向传播算法，自动计算损失函数对所有可训练参数的梯度\n",
    "optimizer.step() #更新参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.1 数据集加载\n",
    "#!pip install datasets\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "raw_datasets=load_dataset(\"glue\",\"mrpc\")\n",
    "#print(raw_datasets)\n",
    "#数据集情况：训练集、验证集和测试集；每个集合有四列：句子1 2 标签 分段\n",
    "\n",
    "#查看数据情况\n",
    "raw_train=raw_datasets['train']\n",
    "#print(raw_train)\n",
    "#print(raw_train.features) #查看标签特征\n",
    "\n",
    "#预处理数据集，我们需要将文本转换为数字\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer=AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_funcation(example):\n",
    "    return tokenizer(example[\"sentence1\"],example[\"sentence2\"],truncation=True)\n",
    "#tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "#tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])\n",
    "tokenized_datasets=raw_datasets.map(tokenize_funcation,batched=True)\n",
    "#print(tokenized_datasets)\n",
    "'''\n",
    "train: Dataset({\n",
    "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
    "        num_rows: 3668\n",
    "    })\n",
    "    填充之后，数据集变为'input_ids', 'token_type_ids', 'attention_mask' 是所需要的矩阵数据\n",
    "'''\n",
    "#动态填充\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator=DataCollatorWithPadding(tokenizer=tokenizer) #填充的是模型\n",
    "sample=tokenized_datasets['train'][:8]\n",
    "sample={k:v for k,v in sample.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "#删除不需要的列 操作方式\n",
    "[len(x) for x in sample['input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorWithPadding(tokenizer=BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "), padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#全流程\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer,DataCollatorWithPadding\n",
    "\n",
    "raw_datasets=load_dataset(\"glue\",\"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_funcation(example):\n",
    "    return tokenizer(example[\"sentence1\"],example[\"sentence2\"],truncation=True)\n",
    "tokenized_datasets=raw_datasets.map(tokenize_funcation,batched=True)\n",
    "data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#3.2 训练\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#!pip install transformers[torch]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#超参数，模型，训练数据加载，开始微调\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments,AutoModelForSequenceClassification\n\u001b[0;32m----> 5\u001b[0m training_args\u001b[38;5;241m=\u001b[39m\u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest-trainer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#!which python\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#!pip list |grep 'accelerate'\u001b[39;00m\n",
      "File \u001b[0;32m<string>:134\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[0;32m~/software/miniconda3/envs/hugging/lib/python3.10/site-packages/transformers/training_args.py:1803\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1801\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[1;32m   1802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[0;32m-> 1803\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[1;32m   1806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_tokens_across_devices:\n",
      "File \u001b[0;32m~/software/miniconda3/envs/hugging/lib/python3.10/site-packages/transformers/training_args.py:2332\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2328\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2329\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   2330\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2331\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 2332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[0;32m~/software/miniconda3/envs/hugging/lib/python3.10/site-packages/transformers/utils/generic.py:74\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     72\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/software/miniconda3/envs/hugging/lib/python3.10/site-packages/transformers/training_args.py:2202\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2202\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2203\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2204\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2205\u001b[0m         )\n\u001b[1;32m   2206\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m accelerator_state_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "#3.2 训练\n",
    "#!pip install transformers[torch]\n",
    "#超参数，模型，训练数据加载，开始微调\n",
    "from transformers import TrainingArguments,AutoModelForSequenceClassification\n",
    "training_args=TrainingArguments(\"test-trainer\")\n",
    "#!which python\n",
    "#!pip list |grep 'accelerate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': 'Rudder was most recently senior vice president for the Developer & Platform Evangelism Business .', 'sentence2': 'Senior Vice President Eric Rudder , formerly head of the Developer and Platform Evangelism unit , will lead the new entity .', 'label': 0, 'idx': 16}\n",
      "However , EPA officials would not confirm the 20 percent figure .\n"
     ]
    }
   ],
   "source": [
    "#作业\n",
    "#训练集的第 15 行元素和验证集的 87 行元素\n",
    "print(raw_train[15])\n",
    "raw_validation=raw_datasets['validation']\n",
    "print(raw_validation[87]['sentence1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
