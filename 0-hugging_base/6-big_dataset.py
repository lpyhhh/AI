#/usr/bin/env python
'''
å¦‚ä½•åœ¨æ–°çš„æ–‡æœ¬è¯­æ–™åº“ä¸Šè®­ç»ƒä¸€ä¸ªç±»ä¼¼äºç»™å®š checkpoint æ‰€ä½¿ç”¨çš„æ–° tokenizer
å¿«é€Ÿ tokenizer çš„ç‰¹æ®ŠåŠŸèƒ½
ç›®å‰ NLP ä¸­ä½¿ç”¨çš„ä¸‰ç§ä¸»è¦å­è¯ tokenization ç®—æ³•ä¹‹é—´çš„å·®å¼‚
å¦‚ä½•ä½¿ç”¨ğŸ¤— Tokenizers åº“ä»å¤´å¼€å§‹æ„å»º tokenizer å¹¶åœ¨ä¸€äº›æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒ
'''
#1 è®­ç»ƒä¸€ä¸ªtokenzierè¯­æ–™åº“
#/home/ec2-user/project/lpy/datasets

#1.1 ä½¿ç”¨python çš„è¯­æ–™åº“
from datasets import load_dataset

raw_datasets = load_dataset(
    "json",
    data_files="/home/ec2-user/project/lpy/datasets/python.jsonl.gz",
    split="train"
)