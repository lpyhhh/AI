{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6e2054",
   "metadata": {},
   "source": [
    "# huggingface 处理大文件\n",
    "\n",
    "1.1 本地文件处理为dataset\n",
    "\n",
    "1.2 分块加载大文件\n",
    "\n",
    "1.3 AI模型如何读取大文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d2f73c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/software/miniconda3/envs/week2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42027ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"train\": \"/home/ec2-user/project/AI/Study/Week1/data/train.csv\", \"validation\": \"/home/ec2-user/project/AI/Study/Week1/data/val.csv\"}\n",
    "down_dataset = load_dataset(\"csv\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97df2915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prot_id', 'seq', 'seq_len', 'pdb_filename', 'ptm', 'mean_plddt', 'emb_filename', 'label', 'source'],\n",
       "        num_rows: 963\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['prot_id', 'seq', 'seq_len', 'pdb_filename', 'ptm', 'mean_plddt', 'emb_filename', 'label', 'source'],\n",
       "        num_rows: 275\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "down_dataset # train表 的 第一行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e53ea6",
   "metadata": {},
   "source": [
    "1.2 分块加载大文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f9dbc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install zstandard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9920445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 这需要几分钟才能运行,所以在你等待的时候去喝杯茶或咖啡 :)\n",
    "data_files = \"https://huggingface.co/datasets/casinca/PUBMED_title_abstracts_2019_baseline/resolve/main/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\n",
    "pubmed_dataset_streamed = load_dataset(\n",
    "    \"json\", \n",
    "    data_files=data_files, \n",
    "    split=\"train\",\n",
    "    streaming=True, # 使用streaming模式来处理大型数据集\n",
    "    cache_dir=\"../test/dataset\"\n",
    "    )\n",
    "#pubmed_dataset\n",
    "#next(iter(pubmed_dataset_streamed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d440b0",
   "metadata": {},
   "source": [
    "1.3 AI模型如何读取大文件\n",
    "\n",
    "首先要把数据集转为embedding，然后导入到模型中去训练？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d0113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载ESM-2模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始计算embedding...\n",
      "第0块：保存64条embedding\n",
      "第1块：保存64条embedding\n",
      "第2块：保存64条embedding\n",
      "第3块：保存64条embedding\n",
      "第4块：保存64条embedding\n",
      "第5块：保存64条embedding\n",
      "第6块：保存64条embedding\n",
      "第7块：保存64条embedding\n",
      "第8块：保存64条embedding\n",
      "第9块：保存64条embedding\n",
      "第10块：保存64条embedding\n",
      "第11块：保存64条embedding\n",
      "第12块：保存64条embedding\n",
      "第13块：保存64条embedding\n",
      "第14块：保存64条embedding\n",
      "第15块：保存3条embedding\n",
      "\n",
      "全部加载完成！共963条embedding\n",
      "目标ID REP|ywg_BAD18935.1 的embedding长度：320\n"
     ]
    }
   ],
   "source": [
    "# 导入必备库\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# ===================== 1. 基础配置（改这里的路径即可） =====================\n",
    "DATA_FILE = \"/home/ec2-user/project/AI/Study/Week1/data/train.csv\"  # 你的CSV文件\n",
    "SAVE_DIR = \"/home/ec2-user/project/AI/Study/Week1/test/dataset/embedding\"  # 保存embedding的文件夹\n",
    "CHUNK_SIZE = 64  # 每次处理64条数据（按需改）\n",
    "MODEL_NAME = \"facebook/esm2_t6_8M_UR50D\"  # ESM-2小模型\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # 自动选GPU/CPU\n",
    "\n",
    "# ===================== 2. 加载模型和分词器（一次性加载） =====================\n",
    "print(\"加载ESM-2模型...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).eval().to(DEVICE)  # 推理模式\n",
    "\n",
    "# 创建保存文件夹\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ===================== 3. 流式读取数据+计算embedding =====================\n",
    "# 加载CSV为流式数据集（不占内存）\n",
    "dataset = load_dataset(\"csv\", data_files=DATA_FILE, split=\"train\", streaming=True)\n",
    "dataset_iter = iter(dataset)  # 转为迭代器\n",
    "chunk_idx = 0  # 块编号\n",
    "\n",
    "print(\"开始计算embedding...\")\n",
    "while True:\n",
    "    # 1. 读取当前块的64条数据，把它保存到列表中\n",
    "    batch_data = []\n",
    "    for _ in range(CHUNK_SIZE):\n",
    "        try:\n",
    "            batch_data.append(next(dataset_iter))  # 读一条数据\n",
    "        except StopIteration:\n",
    "            break  # 数据读完，退出\n",
    "    \n",
    "    if not batch_data:\n",
    "        break  # 没有数据了，结束循环\n",
    "    \n",
    "    # 2. 提取序列和ID，过滤空值\n",
    "    seqs = [item[\"seq\"] for item in batch_data if item[\"seq\"]]\n",
    "    prot_ids = [item[\"prot_id\"] for item in batch_data if item[\"seq\"]]\n",
    "    if not seqs:\n",
    "        chunk_idx += 1\n",
    "        continue\n",
    "    \n",
    "    # 3. 分词+计算embedding\n",
    "    inputs = tokenizer(seqs, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(DEVICE)\n",
    "    with torch.no_grad():  # 关闭梯度，省显存\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # 取<cls>标记的embedding形状：[]批次有 64 条序列，每条序列分词后长度是 100，模型维度 320； [:, 0, :]切片：中间层不要保留64 320 【Numpy 数组是 Python 通用格式，易保存、易处理（比如转列表写入 JSON），而 PyTorch 张量无法直接序列化到文件。\n",
    "    \n",
    "    # 4. 保存结果（JSON格式，ID和embedding一一对应）\n",
    "    emb_dict = {pid: emb.tolist() for pid, emb in zip(prot_ids, embeddings)}\n",
    "    json_path = os.path.join(SAVE_DIR, f\"emb_chunk_{chunk_idx}.json\")\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(emb_dict, f)\n",
    "    \n",
    "    #print(f\"第{chunk_idx}块：保存{len(emb_dict)}条embedding\")\n",
    "    chunk_idx += 1\n",
    "\n",
    "# ===================== 4. 加载embedding（按需使用） =====================\n",
    "def load_all_embeddings(save_dir):\n",
    "    \"\"\"加载所有保存的embedding，返回{prot_id: embedding}的字典\"\"\"\n",
    "    all_emb = {}\n",
    "    for file in os.listdir(save_dir):\n",
    "        if file.startswith(\"emb_chunk_\") and file.endswith(\".json\"):\n",
    "            with open(os.path.join(save_dir, file), \"r\") as f:\n",
    "                all_emb.update(json.load(f))\n",
    "    return all_emb\n",
    "\n",
    "# 加载所有embedding并查看结果\n",
    "all_emb = load_all_embeddings(SAVE_DIR)\n",
    "print(f\"\\n全部加载完成！共{len(all_emb)}条embedding\")\n",
    "print(all_emb[\"REP|ywg_BAD18935.1\"]) #一维矩阵，长320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acdbf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始处理并保存...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pubmed_with_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m开始处理并保存...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 这里的 pubmed_with_embeddings 是我们上一段代码中 map 后的流式对象\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, record \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mpubmed_with_embeddings\u001b[49m):\n\u001b[1;32m     19\u001b[0m     current_batch\u001b[38;5;241m.\u001b[39mappend(record)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# 当累计到指定条数时，保存到本地\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pubmed_with_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# 保存到本地的代码\n",
    "#存为 Parquet 文件（适合后续模型训练）\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 创建存放目录\n",
    "save_dir = \"./pubmed_embeddings\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 假设我们每 1000 条保存一个文件，防止内存溢出\n",
    "batch_size = 1000\n",
    "current_batch = []\n",
    "file_count = 0\n",
    "\n",
    "print(\"开始处理并保存...\")\n",
    "\n",
    "# 这里的 pubmed_with_embeddings 是我们上一段代码中 map 后的流式对象\n",
    "for i, record in enumerate(pubmed_with_embeddings):\n",
    "    current_batch.append(record)\n",
    "    \n",
    "    # 当累计到指定条数时，保存到本地\n",
    "    if (i + 1) % batch_size == 0:\n",
    "        df = pd.DataFrame(current_batch)\n",
    "        file_path = os.path.join(save_dir, f\"part_{file_count}.parquet\")\n",
    "        df.to_parquet(file_path)\n",
    "        \n",
    "        print(f\"已保存 {i+1} 条数据到 {file_path}\")\n",
    "        current_batch = []\n",
    "        file_count += 1\n",
    "        \n",
    "        # 仅作为演示：如果你只想测试前 5000 条，可以 break\n",
    "        if file_count >= 5: \n",
    "            break\n",
    "\n",
    "# 处理剩余的数据\n",
    "if current_batch:\n",
    "    df = pd.DataFrame(current_batch)\n",
    "    df.to_parquet(os.path.join(save_dir, f\"part_{file_count}.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7350a225",
   "metadata": {},
   "source": [
    "HDF5：存储纯数字非常快，但如果你想同时存“论文标题（字符串）”、“作者（列表）”和“向量（数组）”，HDF5 会变得非常麻烦，且效率降低。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
