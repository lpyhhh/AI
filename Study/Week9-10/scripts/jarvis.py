import argparse
import torch
import os
from operator import itemgetter
from collections import deque
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from transformers import EsmTokenizer, EsmForSequenceClassification
from peft import PeftModel

# --- [æ¨¡å— 1] ç”Ÿç‰©æ¨¡å‹æ¥å£ (The "Left Hand") ---
class BioPredictor:
    def __init__(self, lora_path):
        base_model_name = "facebook/esm2_t33_650M_UR50D"
        
        print(f"æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_name}...")
        base_model = EsmForSequenceClassification.from_pretrained(
            base_model_name,
            num_labels=2,
            dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        )
        
        self.tokenizer = EsmTokenizer.from_pretrained(base_model_name)

        print(f"æ­£åœ¨åˆå¹¶ LoRA æƒé‡: {lora_path}...")
        self.model = PeftModel.from_pretrained(base_model, lora_path)
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        self.model.eval()
        
        # åŒ¹é…ä½ åœ¨ 3-lora.py ä¸­çš„æ ‡ç­¾å®šä¹‰
        self.id2label = {0: "Negative (Non-CRESS)", 1: "Positive (CRESS Virus Rep Protein)"}
        print("ç”Ÿç‰©æ¨¡å‹åŠ è½½å®Œæˆã€‚")

    # ã€æ–°å¢ï¼šä¿®å¤ AttributeError çš„æ ¸å¿ƒå‡½æ•°ã€‘
    @torch.no_grad()
    def predict(self, sequence: str):
        """
        å¯¹è›‹ç™½è´¨åºåˆ—è¿›è¡Œåˆ†ç±»æ¨ç†
        """
        # 1. é¢„å¤„ç†ï¼šæ¸…æ´—åºåˆ—å¹¶åˆ†è¯
        inputs = self.tokenizer(
            sequence.strip().upper(), 
            return_tensors="pt", 
            truncation=True, 
            max_length=1024
        ).to(self.device)

        # 2. æ¨¡å‹æ¨ç†
        outputs = self.model(**inputs)
        logits = outputs.logits
        
        # 3. è®¡ç®—æ¦‚ç‡ (Softmax)
        # ä½¿ç”¨ LaTeX è¡¨è¾¾é€»è¾‘ï¼š$$P(i) = \frac{e^{z_i}}{\sum e^{z_j}}$$
        probs = torch.softmax(logits, dim=1)
        pred_idx = torch.argmax(probs, dim=1).item()
        confidence = probs[0][pred_idx].item()

        # 4. è¿”å›è¯†åˆ«å‡ºçš„æ ‡ç­¾å
        label = self.id2label.get(pred_idx, "Unknown")
        return f"{label} (ç½®ä¿¡åº¦: {confidence:.2%})"

def build_agent_logic(db_path, llm_model_name):
    """
    å°†åŸæœ¬åœ¨ main() é‡Œçš„ RAG å’Œ LLM åˆå§‹åŒ–é€»è¾‘æå–å‡ºæ¥
    """
    print(f"æ­£åœ¨åŠ è½½çŸ¥è¯†åº“: {db_path} ...")
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2", 
        model_kwargs={'device': 'cuda'}
    )
    vector_store = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
    retriever = vector_store.as_retriever(search_kwargs={"k": 5})

    print(f"æ­£åœ¨åŠ è½½ LLM: {llm_model_name} ...")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True, bnb_4bit_quant_type="nf4", 
        bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True
    )
    model = AutoModelForCausalLM.from_pretrained(
        llm_model_name, quantization_config=bnb_config, device_map="auto", trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
    
    pipe = pipeline(
        "text-generation", model=model, tokenizer=tokenizer, 
        max_new_tokens=1024, temperature=0.3, repetition_penalty=1.1, return_full_text=False
    )
    llm = HuggingFacePipeline(pipeline=pipe)

    # ä¿®æ”¹ jarvis.py ä¸­çš„ build_agent_logic å‡½æ•°
    template = """<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªé«˜çº§ç”Ÿç‰©ä¿¡æ¯å­¦åŠ©æ‰‹ï¼Œä¸“é—¨è´Ÿè´£ CRESS ç—…æ¯’å’Œè›‹ç™½è´¨åºåˆ—åˆ†æã€‚
ä½ çš„ä»»åŠ¡æ˜¯ç»“åˆæ¨¡å‹é¢„æµ‹ç»“æœå’Œæä¾›çš„æ–‡çŒ®å†…å®¹ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„æŠ¥å‘Šã€‚

# ä»»åŠ¡æŒ‡ä»¤ï¼š
1. æ¨¡å‹é¢„æµ‹ç»“æœï¼š{analysis_result}
2. å‚è€ƒçŸ¥è¯†åº“å†…å®¹ï¼š
{context}

# è¦æ±‚ï¼š
- å¦‚æœçŸ¥è¯†åº“ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯¦ç»†è¯´æ˜è¯¥é¢„æµ‹ç»“æœçš„ç”Ÿç‰©å­¦èƒŒæ™¯ã€‚
- å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›´æ¥ç›¸å…³çš„ä¿¡æ¯ï¼Œè¯·åŸºäºæ¨¡å‹é¢„æµ‹ç»“æœç»™å‡ºä¸€èˆ¬æ€§å»ºè®®ã€‚
- å¿…é¡»å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š{question}
<|im_end|>
<|im_start|>user
{question}
<|im_end|>
<|im_start|>assistant
"""
    prompt = PromptTemplate.from_template(template)

    def format_docs(docs):
        if not docs:
            print("âš ï¸ è­¦å‘Šï¼šçŸ¥è¯†åº“æ£€ç´¢ç»“æœä¸ºç©ºï¼")
            return "æœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³å‚è€ƒèµ„æ–™ã€‚"
        
        print(f"ğŸ“– çŸ¥è¯†åº“æˆåŠŸæ£€ç´¢åˆ° {len(docs)} æ¡ç›¸å…³ç‰‡æ®µ")
        return "\n\n".join([f"[æ¥æº: {d.metadata.get('source', 'æœªçŸ¥')}] {d.page_content}" for d in docs])

    # æ„å»º LCEL é“¾
    agent_chain = (
        {
            "context": itemgetter("search_query") | retriever | format_docs,
            "analysis_result": itemgetter("analysis_result"),
            "question": itemgetter("question")
        }
        | prompt 
        | llm 
        | StrOutputParser()
    )
    return agent_chain

# --- [æ¨¡å— 2] ä¿®æ”¹åçš„ main ç¨‹åº ---
def main():
    parser = argparse.ArgumentParser(description="AI Bio-Agent: Sequence Analysis + Literature Search")
    parser.add_argument("--db_path", type=str, default="/home/ec2-user/project/Week5-6/paper_model")
    parser.add_argument("--bio_model_path", type=str, default="/home/ec2-user/project/Week3-4/results/model/final_lora_model")
    parser.add_argument("--llm_model", type=str, default="Qwen/Qwen2.5-7B-Instruct")
    args = parser.parse_args()

    # åˆå§‹åŒ–å·¦æ‰‹
    bio_predictor = BioPredictor(args.bio_model_path)
    
    # åˆå§‹åŒ–å¤§è„‘ (è°ƒç”¨æ–°æå–çš„å‡½æ•°)
    agent_chain = build_agent_logic(args.db_path, args.llm_model)
    while True:
        user_input = input("\nUser (Input): ").strip()
        if user_input.lower() in ["exit", "quit"]: break
        
        # --- æ™ºèƒ½è·¯ç”±é€»è¾‘ ---
        
        # åˆ¤æ–­è¾“å…¥æ˜¯å¦åƒç”Ÿç‰©åºåˆ— (ç®€å•çš„å¯å‘å¼è§„åˆ™)
        is_sequence = len(user_input) > 20 and not " " in user_input and \
                      (all(c in "ATCGU" for c in user_input.upper()) or \
                       all(c in "ACDEFGHIKLMNPQRSTVWY" for c in user_input.upper()))
        
        analysis_result = "N/A (ç”¨æˆ·æœªæä¾›åºåˆ—)"
        search_query = user_input # é»˜è®¤æœç”¨æˆ·çš„é—®é¢˜
        question = user_input
        
        # åœ¨ jarvis.py çš„ main() æˆ– API.py çš„ ask_jarvis ä¸­ä¿®æ”¹é€»è¾‘
        if is_sequence:
            print(">>> ğŸ”¬ æ£€æµ‹åˆ°ç”Ÿç‰©åºåˆ—ï¼Œå¯åŠ¨åˆ†æå¼•æ“...")
            
            # è·å–åŸå§‹ç»“æœï¼ˆåŒ…å«ç½®ä¿¡åº¦ï¼‰
            full_pred = predictor.predict(user_input) 
            analysis_result = f"æ¨¡å‹é¢„æµ‹ä¸º: {full_pred}"
            
            # ã€æ ¸å¿ƒä¿®æ”¹ã€‘ï¼šæå–æ ‡ç­¾åç§°ï¼Œå»é™¤ç½®ä¿¡åº¦éƒ¨åˆ†ç”¨äºæœç´¢
            # å‡è®¾ full_pred æ˜¯ "Positive (CRESS Virus Rep Protein) (ç½®ä¿¡åº¦: 100.00%)"
            search_label = full_pred.split(" (ç½®ä¿¡åº¦:")[0] 
            search_query = f"{search_label} characteristics and biological function"
            question = f"è¯¥åºåˆ—å·²è¢«é¢„æµ‹ä¸º {search_label}ï¼Œè¯·ç»“åˆæ£€ç´¢åˆ°çš„æ–‡çŒ®è¯¦ç»†åˆ†æå…¶ç”Ÿç‰©å­¦æ„ä¹‰ã€‚"
        
        else:
            print(">>> ğŸ“– æ£€æµ‹åˆ°æ–‡æœ¬æé—®ï¼Œå¯åŠ¨æ£€ç´¢æ¨¡å¼...")

        # Step C: è°ƒç”¨å¤§è„‘ (LLM + RAG)
        print(">>> æ­£åœ¨ç”ŸæˆæŠ¥å‘Š...")
        print("\nAssistant (A): ", end="", flush=True)
        
        try:
            # æµå¼è¾“å‡º
            for chunk in agent_chain.stream({
                "analysis_result": analysis_result,
                "search_query": search_query,
                "question": question
            }):
                print(chunk, end="", flush=True)
            print("\n")
        except Exception as e:
            print(f"Error: {e}")

if __name__ == "__main__":
    main()