---
### **测试题：LoRA 微调 ESM-2 综合评估**

---

#### **第一部分：原理与理论基础**

**1. 请用数学公式和通俗语言解释 LoRA (Low-Rank Adaptation) 的核心原理。**
- 它是如何将全量微调转化为低秩微调的？
- 预训练的权重矩阵 $W$ 和微调过程中的增量 $\Delta W$ 分别是如何表示的？

**2. LoRA 相比于全量微调和 Adapter 方法，有哪些显著的优缺点？**
- 为什么说 LoRA 在部署推理阶段具有优势？（提示：参数合并）
- LoRA 的一个潜在局限是什么？（提示：假设可训练参数的低秩特性）

**3. 在 LoRA 微调中，超参数 `r` (rank) 和 `lora_alpha` (scaling factor) 分别起到了什么作用？**
- 公式 `scaling = lora_alpha / r` 是如何影响最终输出的？（“低秩矩阵”对原模型参数的影响程度；对$\Delta W$放大缩小加到模型上）
- 如果将 `r` 设置得过大（例如接近原矩阵维度），会发生什么？

---

#### **第二部分：代码实现与配置 (LoraConfig & ESM-2)**

**4. 在针对 ESM-2 模型配置 `LoraConfig` 时，`target_modules` 参数应该设置为什么？**
- 为什么 ESM-2（作为一个 Transformer 模型）主要微调这些模块？（更接近于模型开始，如果从开始就介入，对模型产生的效果更好。对于dense，是结果输出的地方，更能解决问题。）
- 如果不指定 `target_modules` 或者指定错误（如只指定了 LayerNorm），会出现什么后果？（本质上考察LoRA在微调时，对数据是如何操作的？如果全0，那么就是没有微调；如果只用在LM层，标准化层，两个参数。对模型影响小）

**5. 请详细描述 `get_peft_model(base_model, peft_config)` 这一步操作在底层做了什么？**
- 它是替换了原模型的层，还是在原层旁边插入了新结构？（数据流向来说明这个问题：数据流入两个方向，原始不动的模型，新创建的qkv。对这个新qkv权重计算，如果计算到合适的，把这个结果加入到原始qkv）
- 如何通过代码（如 `print_trainable_parameters` 函数）验证你的 LoRA 设置确实大幅减少了可训练参数数量？（请估算一下你的实验中参数量减少了百分之多少？）（print_trainable_parameters输出占比）

**6. 在编写 `train_lora.py` 时，数据预处理阶段有什么特殊注意事项？**
- ESM-2 模型对输入数据的格式（如 Tokenizer）有什么特殊要求？（例如：特殊的起始/结束 token，是否需要截断/填充）（碱基替换，过长扔掉）
- 在 DataLoader 构建时，Labels 通常是如何处理的？（例如：在蛋白质二级结构预测或结合亲和力预测任务中，Label 的 shape 是怎样的？）（数据构建时，如何加标签？残基任务（多对多）：填充相关标签[batch,max_len]。序列任务（多对一）：不用填充，单batch）

---

#### **第三部分：实验设计与验证逻辑**

**7. 你是如何设计实验来证明 LoRA 微调后的模型“具有优势”的？请描述你的对比组设置。**
- 你对比了哪些指标？（准确率、F1-score、召回率？）
- 除了准确率，你如何验证“时间和准确率的提升”？请具体说明你是如何记录和对比训练时间/推理时间的。（看loss曲线。从序列输入开始到模型保存完成。）

**8. 在实验结果中，LoRA 微调通常会在训练集上表现略逊于全量微调，但在验证集上表现接近甚至更好。请解释这一现象的原因。**
- 这涉及到什么机器学习概念？（过拟合 与 泛化能力 之间的权衡，以及 奥卡姆剃刀）
- LoRA 的低秩特性起到了什么作用？（模型只记住关键信息 隐式正则化，对于细节方面可以稍微忽略）

**9. 如果你的 LoRA 微调模型训练完成后，准确率远低于预期（例如接近随机猜测），你会从哪些维度进行 Debug？**
- 列举至少 3 个可能的排查方向（测试样本填充情况（Token 偏移，Padding与Label标签位置），学习率稳定，target_modules，损失和任务匹配）

---

#### **第四部分：深度思考**

**10. ESM-2 模型参数量巨大（例如 ESM-2-650M 有 6.5 亿参数）。在使用 LoRA 微调时，如果显存（VRAM）依然不足，除了减小 Batch Size，你还可以结合哪些其他技术来解决问题？**
- 提示：结合混合精度训练（FP16/BF16）或量化（4bit/8bit）来谈谈。（r，QoRA）

---

### **自我评估标准（参考答案要点）**

- **Q1:** 能写出 $W = W_0 + \Delta W = W_0 + BA$，且说明 $B$ 和 $A$ 的维度远小于 $W_0$ 的维度。
- **Q2:** 优点：参数少、显存占用低、无额外推理延迟（因为可合并）。缺点：对于某些复杂任务，低秩假设可能不成立，限制了模型的表达上限。
- **Q3:** `r` 决定矩阵的秩（容量），`alpha` 决定缩放权重。`r` 太大则失去了 LoRA 的轻量化意义。
- **Q4:** ESM-2 应该设置 `target_modules=["query", "key", "value", "dense"]` (或类似 Attention 层的 Linear 层)。因为 Attention 层包含了模型的主要知识，且微调这里最有效。如果只微调 MLP，效果通常不如微调 Attention。
- **Q5:** 它冻结了原参数，并在 Linear 层旁插入 A 和 B 矩阵。验证方法是打印模型参数，检查 `requires_grad=True` 的比例。
- **Q6:** ESM-2 通常需要特殊的 Tokenizer（如 facebook/esmTokenizer），且输入通常是蛋白质序列字符串。Labels 需要根据具体任务转换为 tensor。
- **Q7:** 对比组应包括：全量微调、不微调（直接用预训练模型）。优势体现：LoRA 在准确率接近全量微调的前提下，训练时间大幅缩短，显存占用大幅降低。
- **Q8:** 现象：LoRA 泛化性更好。原因：全量微调容易过拟合，LoRA 通过限制参数量的低秩更新，起到了正则化的作用。
- **Q9:** 检查点：1. 学习率是否过大/过小（LoRA 通常需要比全量微调稍大的学习率）；2. `r` 是否太小导致模型欠拟合；3. 数据的 Label 是否对应正确；4. 是否真的只有 LoRA 参数在训练（基础模型是否被意外冻结或解冻）。
- **Q10:** 可以使用 `BitsAndBytesConfig` 进行 4bit/8bit 加载，结合 `torch.cuda.amp` 进行混合精度训练。



---
### **进阶测试题：LoRA 部署、优化与 ESM-2 细节**
#### **第一部分：模型部署与推理优化**
**1. 在 LoRA 训练完成后，`merge_and_unload()` 方法的作用是什么？**
*   请从模型结构和推理速度两个角度解释它的作用。
*   执行合并后，原本的 `lora_A` 和 `lora_B` 矩阵去哪了？
**2. 为什么 LoRA 可以实现“零推理延迟”，而 Adapter 方法通常会有推理延迟？**
*   请对比两者的计算图结构差异。（提示：串行计算 vs 参数合并）
#### **第二部分：显存与效率优化**
**3. 什么是 QLoRA (Quantized LoRA)？它相比标准 LoRA 在显存占用上有何不同？**
*   解释 **NF4 (NormalFloat4)** 数据类型的作用。
*   在微调 ESM-2 (尤其是 650M 或 3B 参数版本) 时，QLoRA 是如何让显存需求大幅降低的？
**4. 在 ESM-2 的微调中，为什么通常不将 `embeddings` (词嵌入层) 放入 `target_modules`？**
*   从参数量的角度分析（Embedding 层通常占总参数量的多少？）。
*   如果冻结 Embedding 层，对于处理包含稀有氨基酸或非标准氨基酸的序列会有什么影响？
#### **第三部分：超参数微调与训练稳定性**
**5. `LoraConfig` 中的 `lora_dropout` 参数是如何起作用的？**
*   它是将 Dropout 应用在 LoRA 矩阵的输入上，还是输出上？
*   如果将 `lora_dropout` 设为 0.0，在数据量较小的情况下可能会导致什么后果？
**6. 在 LoRA 微调中，使用学习率调度器时，为什么 "Warmup"（预热阶段）特别重要？**
*   考虑到 LoRA 的初始化策略（B 矩阵全为 0），训练初期的梯度流有什么特点？
*   如果一开始就使用很大的学习率，会发生什么？
#### **第四部分：架构扩展**
**7. (思考题) 假设你需要同时微调蛋白质的“序列分类”和“残基级标签”（例如同时预测稳定性位点和整体稳定性），你会如何设计 LoRA 策略？**
*   你是使用一个共享的 LoRA 配置，还是针对不同的输出头设置不同的 `r` 值或 `alpha` 值？
*   在 ESM-2 的不同深度层，是否需要设置不同的 LoRA 策略？（提示：底层学通用特征，顶层学任务特征）。
---
### **参考答案与解析要点**
**1. `merge_and_unload()`**
*   **作用**：将训练好的低秩增量 $\Delta W = \frac{\alpha}{r}BA$ 直接加到冻结的预训练权重 $W_0$ 上，得到 $W_{new} = W_0 + \Delta W$。
*   **结果**：
    *   **结构上**：模型变回了标准的 Transformer 结构（不再包含 LoRA 的旁路分支）。
    *   **速度上**：推理时不再需要计算 $BA$ 的乘法，也没有额外的加法操作，因此推理速度与原模型完全一致，无额外开销。
*   **矩阵去向**：`lora_A` 和 `lora_B` 被删除，释放了显存。
**2. LoRA vs Adapter (推理延迟)**
*   **Adapter**：在 Transformer 层之间插入了新的小型全连接层。推理时，数据必须流经这些层（串行结构）。这增加了 FLOPs（浮点运算次数）和内存访问开销，导致推理变慢。
*   **LoRA**：结构上是并行旁路。在推理阶段，通过 `merge` 操作将旁路权重合并回主干，实际上消除了旁路。因此，推理时计算图没有任何增加。
**3. QLoRA**
*   **原理**：在微调前，先将基础模型（Base Model）以 4-bit 量化形式加载到显存中，而 LoRA 适配器部分依然保持 16-bit 或 32-bit 浮点数进行训练。
*   **NF4**：一种针对正态分布权重优化的 4-bit 数据类型，能最大程度减少量化带来的精度损失。
*   **显存差异**：标准 LoRA 需要加载全量精度的 Base Model（例如 650M 参数约需 2.5GB 显存），QLoRA 只需要加载 4-bit 的 Base Model（约需 0.6-0.8GB），大幅降低了门槛。
**4. 关于 Embeddings**
*   **参数量**：Embedding 层通常很大（词表大小 × 隐藏层维度），但如果只微调它而不微调上层，往往无法捕捉复杂的上下文依赖关系。且对于 ESM-2，冻结 Embedding 是常见的做法，因为氨基酸的理化性质是相对固定的。
*   **稀有氨基酸**：如果冻结 Embedding，模型对稀有氨基酸的表示可能不准确。但在微调下游任务时，通常认为上层的 Attention 机制足以通过上下文来弥补 Embedding 的不足。
**5. `lora_dropout`**
*   **作用位置**：应用在 LoRA 分支的输出上（即在 $BA$ 计算之后，Scaling 之前或之后）。
*   **后果**：设为 0.0 意味着 LoRA 分支没有任何正则化。在小数据集上，LoRA 很容易过拟合（即 $BA$ 学到了特定的噪声），导致验证集性能下降。
**6. Warmup 的重要性**
*   **初始化特点**：LoRA 初始化时 $B=0$，这意味着训练开始时，梯度完全由预训练模型的主干路径决定，LoRA 分支的梯度非常小。
*   **如果不预热**：如果一开始就用大学习率，可能会导致预训练模型的主干权重（虽然设为不更新，但在某些混合精度或特定优化器设置下可能影响 momentum 计算）或者 LoRA 的 A 矩阵更新过剧烈，破坏了原本的预训练特征分布，导致训练震荡或 Loss 爆炸。
**7. 多任务 LoRA 策略 (开放题)**
*   **配置策略**：通常可以使用一个共享的 LoRA 配置来捕捉通用的蛋白质特征变化。
*   **层级策略**：
    *   **底层 Layers**：建议 Rank `r` 设小一点，或者冻结不微调，因为底层主要提取氨基酸局部模式，这些通常是通用的。
    *   **顶层 Layers**：建议 Rank `r` 设大一点，增加模型容量，因为顶层负责捕捉与具体任务（如结合位点、稳定性）相关的高级语义特征。
---
