{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb1b34c",
   "metadata": {},
   "source": [
    "AI流程 数据处理 数据导入 模型构建 模型测试\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac1b803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using divice cuda:0\n"
     ]
    }
   ],
   "source": [
    "#/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Feature Extraction (FE)\n",
    "重新生成 嵌入向量，也就是预处理咯\n",
    "\"\"\"\n",
    "import time\n",
    "import h5py\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from transformers import T5EncoderModel,T5Tokenizer\n",
    "\n",
    "#GPU 使用第一个gpu或cpu判定\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "print(\"using divice {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ac86a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cluster4_BIO|CRR141969_orf1998': 'MDKITTVNVGTEKDPQLVQIGSTLSLEERERLVALLKDFKDVFAWSYEDMPGINPEIVQHQIPLDPKARPIKQKLRRIRPDWAIKIKEEVTKQIDAGFLLVSEYLTWLANIVPVPKKDGKI',\n",
       " 'cluster4_BIO|CRR141969_orf2000': 'VGTETAASTEMENATSIGIEGTASIGTFMPKPDRCLSAADGIWWEDDDLCLAHTDEDWKGNQPDDTWYIGEVDHMTRSGRYFKPPHLDQPEASGKDREAEKQKEKQLEDEAVLRQLKKIQADISIWGLLMASRVHRQAVLSAMDKAKLSIDTTPEQLVGLVFSRGATPMLTFSDKELPPEGANHNKPLYISVECRDKWIPVVLVDTGSAINICPARTAYAIGLKIADFVPTAQVIRAYDNTSREVMGTVKIQTKVGPGQHEIDFHVLEVPATFNLFLGRPWLHQVKAVSSTLHQVVKYPYGKGIAIVFRNLSIHPPPEVSTPVLEIEHGMEDVFISGFTLAEAWVVQDIMAANEGVYVSAQAVYLMNKLEHIAGMGLEKSGRKGVAALAEVPHNPHTFGLGYLPTKEDWIRKGK',\n",
       " 'cluster4_BIO|CRR141873_orf239681': 'VRQKKAKNSVLISMHDVQSIESAERAYIGSLTVQPTLLGKVSQMYNLYEFLVSKLDDLVVDLIDDCPTDWSVDSDGGLWFKGRLGVPNIVELRKKIMDDTHRSRYTIHLGGTKIYHDLKRTFWWEGMKKDVGEYVSRCYVCQ'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据处理\n",
    "def read_fasta(fasta_path):\n",
    "    \"\"\"\n",
    "    读取 FASTA 文件，返回一个 dict：{uniprot_id: sequence}。\n",
    "    header 行以 '>' 开始，取整行作为 ID，但将 '/' 和 '.' 替换为 '_'（因为 HDF5 dataset 名中这些字符可能有问题）。\n",
    "    读取序列行时去掉所有空白并转为大写，移除 '-'（gap）。\n",
    "    注意：如果同一个 header 出现多行序列，会拼接成一条\n",
    "    \"\"\"\n",
    "    sequences = dict()\n",
    "    with open(fasta_path,'r') as fasta:\n",
    "        for line in fasta:\n",
    "            if line.startswith('>'):#检查字符串是否以指定子字符串开头\n",
    "                uniprot_id = line.replace('>','').strip() # 去除字符串首尾的空白字符\n",
    "                uniprot_id = uniprot_id.replace('/','_').replace('.',\"_\")# 两种字符用_替代\n",
    "                sequences[uniprot_id]=''#赋值给字典\n",
    "            else: # 字典 赋值\n",
    "                sequences[uniprot_id]+=''.join(line.split()).upper().replace(\"-\",\"\") # 字典，每行 去空 大写 替代\n",
    "    return sequences\n",
    "read_fasta(\"/home/ec2-user/project/test.fa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b27219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型构建\n",
    "\"\"\"\n",
    "加载模型，并缓存到指定目录，如果 device 是 CPU 则把模型 cast 到 float32，返回 model（eval 模式）和 vocab（tokenizer）\n",
    "\"\"\"\n",
    "def get_T5_model(model_dir, transformer_link = \"Rostlab/prot_t5_xl_half_uniref50-enc\"):\n",
    "    print(\"Loading {}\".format(transformer_link))\n",
    "    if model_dir is not None:\n",
    "        print(\"########################## model location \")\n",
    "        print(\"Loading cached model from: {}\".format(model_dir))\n",
    "        print(\"##########################\")\n",
    "    \n",
    "    model = T5EncoderModel.from_pretrained(transformer_link,cache_dir=model_dir)\n",
    "\n",
    "    # only cast to full-precision if no GPU is available\n",
    "    if device==torch.device(\"cpu\"):\n",
    "        model.to(torch.float32)\n",
    "        #问题：为什么这样做？（GPU 靠低精度提性能、省资源；CPU 靠高精度保正确、保速度；同时用一套代码适配所有环境，降低维护成本。）现代 GPU（如 NVIDIA Ampere/Turing 架构）原生支持 FP16（半精度）、BF16 等低精度计算，低精度能大幅提升计算速度（吞吐量）、降低显存占用，且 GPU 的硬件浮点单元对低精度的数值稳定性有优化，因此无需强制转 FP32。也就是 从应用环境入手，低精度可以对大规模数据进行处理，但消耗资源多，因此发明 低精度；低精度是全精度的一半，无论是显存还是速度，但因cpu默认是32位，因此要把它转为FP32。（如果是x64架构呢？架构和精度不同，不能混淆）（这些名词是啥？https://zhuanlan.zhihu.com/p/673708074 ）\n",
    "    \n",
    "    # load module and token\n",
    "    model = model.to(device).half()\n",
    "    model = model.eval() #评估模式，no_grad()无梯度模式\n",
    "    vocab = T5Tokenizer.from_pretrained(transformer_link,do_lower_case=False) #不强制转换大小\n",
    "    return model,vocab\n",
    "\n",
    "def get_embeddings(\n",
    "    seq_path,\n",
    "    model_dir,\n",
    "    emb_path, #output H5 of embedding vector\n",
    "    per_protein,#布尔值：True = 生成「整个蛋白的嵌入」（均值池化）；False = 保留「每个残基的嵌入」 (问题：True → 做均值池化（mean-pooling）：把所有残基的向量求平均，得到整个蛋白的 1 个向量 ; 不做池化，原样返回，形状是「序列长度 × 向量维度」，即每个残基 1 个向量)\n",
    "    max_residues=4000,#单批次累计残基数上限（避免显存不够\n",
    "    max_seq_len=4000,#序列长度阈值（超过则单独处理\n",
    "    max_batch=1, #单批次最多序列\n",
    "):\n",
    "#报错：SyntaxError: non-default argument follows default argument 有默认参数的顺序\n",
    "    \"\"\"\n",
    "    输入数据，进行运算。\n",
    "    \"\"\"\n",
    "    seq_dict=dict()# 读取的数据保存\n",
    "    emb_dict=dict()#最终的嵌入，用作调试\n",
    "    # 1 加载序列 和 模型处理\n",
    "    seq_dict=read_fasta(seq_path) #return sequences dict\n",
    "    model,vocab=get_T5_model(model_dir) # load model and return model and vocab\n",
    "    # test of output\n",
    "    print('########################################')\n",
    "    print('example sequence {}\\n{}'.format(\n",
    "    next(iter(seq_dict.keys())), \n",
    "    next(iter(seq_dict.values()))\n",
    "    ))\n",
    "\n",
    "    # 统计字典序列基础信息：平均长度、超长序列数量 []列表推导式\n",
    "    avg_length = sum([len(seq) for _,seq in seq_dict.items()]) / len(seq_dict)\n",
    "    n_long = sum([1 for _,seq in seq_dict.items() if len(seq)>max_seq_len])\n",
    "    #按序列长度降序排序（先处理长序列，避免长序列扎堆导致OOM） 定位value，统计长度，排序\n",
    "    sorted(seq_dict,key=lambda kv:len(kv[1]),reverse=True) #lambda 变量，计算\n",
    "\n",
    "    print(\"Average sequence length: {}\".format(avg_length))\n",
    "    print(\"Number of sequences {}: {}\".format(max_seq_len, n_long))\n",
    "\n",
    "    ## 批量构建运行，避免OOM\n",
    "    start = time.time()\n",
    "    batch=list()# 临时存当前批次的序列（格式：[(ID, 空格分隔的序列, 长度), ...]）\n",
    "    #遍历seq_dict，替换稀有字符，统计长度，加上空格，加入到batch中，计算当前批次累积残基数，处理批次\n",
    "    for seq_idx,(pdb_id,seq) in enumerate(seq_dict.items(),1): #三个变量\n",
    "        seq=seq.replace('U','X').replace('Z','X').replace('O','X')\n",
    "        seq_len=len(seq)\n",
    "        seq=' '.join(list(seq))\n",
    "        batch.append((pdb_id,seq,seq_len))\n",
    "        #print(batch)\n",
    "        n_res_batch = sum([s_len for _,_,s_len in batch]) + seq_len #for循环导致最后一个seq_len没加 , n_res_batch作用：一共累加多少个残基\n",
    "\n",
    "        # batch 处理\n",
    "        # 1. 批次序列数≥max_batch；2. 累计残基数≥max_residues；3. 遍历到最后一个序列；4. 当前序列是超长序列（单独处理）\n",
    "        if len(batch)>=max_batch or n_res_batch>=max_residues or seq_idx==len(seq_dict) or seq_len>max_seq_len:\n",
    "            # 拆分批次：把batch里的ID、序列、长度分别拆成三个元组\n",
    "            pdb_ids,seqs,seq_lens = zip(*batch) # 转制变量\n",
    "            batch = list()\n",
    "            #以上，序列处理完成，序列进行embedding和 training model\n",
    "\n",
    "            ########## 分词器编码：把序列转成模型能识别的数字ID ##########\n",
    "            # batch_encode_plus：批量编码；add_special_tokens=True：加T5的特殊token（如<s>/</s>）；padding=\"longest\"：按批次最长序列补零\n",
    "            token_encoding = vocab.batch_encode_plus(seqs,add_apecial_tokens=True,padding=\"longest\")\n",
    "            input_ids = torch.tensor(token_encoding['input_ids']).to(device)\n",
    "            attention_mask = torch.tensor(token_encoding['attention_mask']).to(device)\n",
    "\n",
    "            ########### 模型推理：生成嵌入 ##########\n",
    "            #模型推理，失败跳过\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    embedding_repr = model(input_ids,attention_mask=attention_mask)\n",
    "            except:\n",
    "                print(\"RuntimeError during embedding for {} (L={}). Try lowering batch size. \".format(pdb_id, seq_len) + \"If single sequence processing does not work, you need more vRAM to process your protein.\")\n",
    "                continue\n",
    "\n",
    "            ## 4 嵌入后处理：切分 padding + 池化，存到字典\n",
    "            # ########## 嵌入处理：去掉padding，可选均值池化 ##########\n",
    "            # embedding_repr.last_hidden_state.shape = [批次大小, 序列长度（含padding）, 嵌入维度]\n",
    "            # 遍历批次内的每个序列\n",
    "            for batch_idx, identifier in enumerate(pdb_ids):\n",
    "                s_len = seq_lens[batch_idx]#真实长度\n",
    "                emb=embedding_repr.last_hidden_state[batch_idx,:s_len]#保留这个长度\n",
    "\n",
    "                # 如果需要蛋白级嵌入（per_protein=True），对残基嵌入做均值池化（dim=0：按序列长度维度求平均）\n",
    "                if per_protein:\n",
    "                    emb = emb.mean(dim=0)\n",
    "                # 打印第一个嵌入的形状（方便调试）\n",
    "                if len(emb_dict) == 0:\n",
    "                    print(\"Embedded protein {} with length {} to emb. of shape: {}\".format(identifier, s_len, emb.shape))\n",
    "                #把张量从 GPU 挪到 CPU，转成 numpy 数组（脱离 PyTorch 计算图），避免显存泄漏。\n",
    "                emb_dict[identifier]=emb.detach().cpu().numpy().squeeze()\n",
    "    end = time.time()\n",
    "    with h5py.File(str(emb_path),'w') as hf:\n",
    "        for sequence_id,embedding in emb_dict.items():\n",
    "            hf.create_dataset(sequence_id, data=embedding)\n",
    "    # 打印统计信息（方便评估效率）\n",
    "    print('\\n############# STATS #############')\n",
    "    print('Total number of embeddings: {}'.format(len(emb_dict)))\n",
    "    print('Total time: {:.2f}[s]; time/prot: {:.4f}[s]; avg. len= {:.2f}'.format( \n",
    "            end-start, (end-start)/len(emb_dict), avg_length))\n",
    "    return True\n",
    "\n",
    "\n",
    "#get_embeddings(seq_path=\"/home/ec2-user/project/test.fa\",emb_path=\"/home/ec2-user/project/emb\",model_dir=\"/home/ec2-user/project/model\",per_protein=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e150dcd5",
   "metadata": {},
   "source": [
    "序列embedding需要考虑的几个方面：  \n",
    "1 序列过长：更细致的 fallback（比如分段或 sliding window）  \n",
    "2 保存模型：占用大量显存。可以考虑更细粒度控制（比如按长度分桶）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c08d212",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ArgumentParser' object has no attribute 'add_argment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     get_embeddings(seq_path,emb_path,model_dir,per_protein\u001b[38;5;241m=\u001b[39mper_protein)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 14\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m---> 14\u001b[0m     parser \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_arg_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args()\n\u001b[1;32m     17\u001b[0m     seq_path \u001b[38;5;241m=\u001b[39m Path(args\u001b[38;5;241m.\u001b[39minput)\n",
      "Cell \u001b[0;32mIn[33], line 7\u001b[0m, in \u001b[0;36mcreate_arg_parser\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mUsing argparse to running for this py coding\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m parser \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mArgumentParser(description\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt5_embedder.py creates T5 embeddings for a given text \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m file containing sequence(s) in FASTA-format.\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_argment\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-i\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--input\u001b[39m\u001b[38;5;124m\"\u001b[39m,required\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m,help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA path to a fasta-formatted text file containing protein sequence(s).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argment(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-o\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--output\u001b[39m\u001b[38;5;124m'\u001b[39m,required\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m,help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA path for saving the created embeddings as NumPy npz file.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argment(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--model\u001b[39m\u001b[38;5;124m'\u001b[39m,required\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m,help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA path to a directory holding the checkpoint for a pre-trained model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ArgumentParser' object has no attribute 'add_argment'"
     ]
    }
   ],
   "source": [
    "# 模型测试\n",
    "def create_arg_parser():\n",
    "    \"\"\"\n",
    "    Using argparse to running for this py coding\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=('t5_embedder.py creates T5 embeddings for a given text '+' file containing sequence(s) in FASTA-format.'))\n",
    "    parser.add_argment(\"-i\",\"--input\",required=True,type=str,help='A path to a fasta-formatted text file containing protein sequence(s).')\n",
    "    parser.add_argment('-o','--output',required=True,type=str,help='A path for saving the created embeddings as NumPy npz file.')\n",
    "    parser.add_argment('--model',required=False,type=str,help='A path to a directory holding the checkpoint for a pre-trained model')\n",
    "    parser.add_argment('--per_protein',type=int,default=0,help=\"Whether to return per-residue embeddings (0: default) or the mean-pooled per-protein representation (1).\")\n",
    "    return parser\n",
    "\n",
    "def main():\n",
    "    parser = create_arg_parser()\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    seq_path = Path(args.input)\n",
    "    emb_path = Path(args.output)\n",
    "    model_dir = Path(args.model) if args.model is not None else None\n",
    "    per_protein = False if int(args.per_protein) == 0 else True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74384d48",
   "metadata": {},
   "source": [
    "# 补充\n",
    "数据是怎么流动？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
